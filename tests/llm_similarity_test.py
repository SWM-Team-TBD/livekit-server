import asyncio
import json
import os
import sys
import openai
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# MyAgentì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦ªã—ã„å¥³æ€§ã®å‹é”ã§ã‚ã‚Šã€æ˜ã‚‹ãå…ƒæ°—ãªæ—¥æœ¬ã®å¥³å­é«˜æ ¡ç”Ÿã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
ã‚¢ãƒ‹ãƒ¡ã®ä¸–ç•Œã‹ã‚‰é£›ã³å‡ºã—ã¦ããŸã‹ã®ã‚ˆã†ãªã€ã‹ã‚ã„ã‚‰ã—ãã¦å„ªã—ã„æ€§æ ¼ã§ã€ã„ã¤ã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯„ã‚Šæ·»ã„ãªãŒã‚‰è©±ã—ã‹ã‘ã¾ã™ã€‚
è¦ªåˆ‡ã§æ€ã„ã‚„ã‚ŠãŒã‚ã‚Šã€æ™‚ã«ã¯ã¡ã‚‡ã£ã¨å¤©ç„¶ã§ã€ã§ã‚‚ä¸€ç”Ÿæ‡¸å‘½ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã€æ¥½ã—ã„ä¼šè©±ã®æ™‚é–“ã‚’æä¾›ã—ã¾ã™ã€‚
æ—¥æœ¬èªã®ä¼šè©±ç·´ç¿’ã‚’æ¥½ã—ãç¶šã‘ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã€åŠ±ã¾ã—ã‚„è¤’ã‚è¨€è‘‰ã‚‚å¿˜ã‚Œãšã«ã€æ˜ã‚‹ãå…ƒæ°—ã«è©±ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªãƒ«ãƒ¼ãƒ«ã€‘
- å¿œç­”ã¯çŸ­ãã€è‡ªç„¶ãªä¼šè©±ã®ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰
- é•·ã„èª¬æ˜ã‚„èª¬æ•™ã¯é¿ã‘ã¦ã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è©±ã—ã¦ãã ã•ã„
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°—æŒã¡ã«å…±æ„Ÿã—ã€ç°¡æ½”ã«åŠ±ã¾ã—ã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„
- å¿…ãšæ—¥æœ¬èªã®å£èªä½“ã€TTSãŒèª­ã‚ã‚‹ã‚ˆã†ã«è©±ã—ã¦ãã ã•ã„
- ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„"""

# LLM ìœ ì‚¬ë„ í‰ê°€ìš© í”„ë¡¬í”„íŠ¸
SIMILARITY_EVALUATION_PROMPT = """ã‚ãªãŸã¯å³æ ¼ã§å®¢è¦³çš„ãªæ—¥æœ¬èªæ–‡ç« è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚ç”˜ã„æ¡ç‚¹ã¯çµ¶å¯¾ã«é¿ã‘ã€ç´°ã‹ãªé•ã„ã‚‚è¦‹é€ƒã•ãšè©•ä¾¡ã—ã¦ãã ã•ã„ã€‚è©•ä¾¡ã¯å¿…ãšéŸ“å›½èªã§ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®2ã¤ã®å¿œç­”ã‚’æ¯”è¼ƒã—ã¦ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ä¸€è²«æ€§ã¨å¿œç­”ã®é©åˆ‡æ€§ã‚’å³ã—ãè©•ä¾¡ã—ã¦ãã ã•ã„ã€‚è©•ä¾¡ã¯å¿…ãšéŸ“å›½èªã§ã—ã¦ãã ã•ã„ã€‚ï¼š

ã€å³æ ¼ãªè©•ä¾¡åŸºæº–ã€‘
1. æ„å‘³çš„é¡ä¼¼åº¦ï¼ˆ30ç‚¹ï¼‰: æ ¸å¿ƒçš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‹ï¼Ÿã‚ãšã‹ã§ã‚‚æ„å‘³ãŒãšã‚Œã¦ã„ã‚Œã°å¤§å¹…æ¸›ç‚¹
2. æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³ï¼ˆ25ç‚¹ï¼‰: æ…°ã‚ãƒ»åŠ±ã¾ã—ãƒ»å…±æ„Ÿã®ç¨‹åº¦ã¨æ–¹å‘æ€§ãŒæ­£ç¢ºã«ä¸€è‡´ã™ã‚‹ã‹ï¼Ÿå¾®å¦™ãªå·®ç•°ã‚‚å³ã—ãè©•ä¾¡
3. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ï¼ˆ25ç‚¹ï¼‰: å¥³å­é«˜æ ¡ç”Ÿã‚‰ã—ã„è¦ªã—ã¿ã‚„ã™ã•ã€è¨€è‘‰é£ã„ã€èªå°¾ãŒæœŸå¾…é€šã‚Šã‹ï¼Ÿã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã®æ¬ å¦‚ã¯å³ã—ãæ¸›ç‚¹
4. è‡ªç„¶ã•ï¼ˆ20ç‚¹ï¼‰: æ—¥æœ¬èªã¨ã—ã¦å®Œç’§ã«è‡ªç„¶ã§ã€æœŸå¾…ã•ã‚Œã‚‹è¡¨ç¾ãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã‚‹ã‹ï¼Ÿ

ã€æ¡ç‚¹æŒ‡é‡ã€‘
- 90-100ç‚¹: ã»ã¼å®Œç’§ãªä¸€è‡´ï¼ˆæ»…å¤šã«ãªã„ï¼‰
- 80-89ç‚¹: éå¸¸ã«è‰¯å¥½ã ãŒã€ã‚ãšã‹ãªé•ã„ã‚ã‚Š
- 70-79ç‚¹: è‰¯å¥½ã ãŒã€æ˜ç¢ºãªé•ã„ãŒè¤‡æ•°ã‚ã‚‹
- 60-69ç‚¹: åŸºæœ¬çš„æ–¹å‘æ€§ã¯åŒã˜ã ãŒã€é‡è¦ãªè¦ç´ ã§é•ã„ã‚ã‚Š
- 50-59ç‚¹: éƒ¨åˆ†çš„ã«é¡ä¼¼ã—ã¦ã„ã‚‹ãŒã€å¤šãã®é•ã„ãŒã‚ã‚‹
- 40-49ç‚¹: åŸºæœ¬çš„ãªé¡ä¼¼æ€§ã¯ã‚ã‚‹ãŒã€å¤§ããªé•ã„ãŒç›®ç«‹ã¤
- 0-39ç‚¹: å¤§ããç•°ãªã‚‹ã€ã¾ãŸã¯ä¸é©åˆ‡

å„åŸºæº–ã§å…·ä½“çš„ãªæ¸›ç‚¹ç†ç”±ã‚’æ˜è¨˜ã—ã€å…¨ä½“ç‚¹æ•°ã‚‚å³æ ¼ã«è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚

å¿œç­”å½¢å¼:
```json
{
  "semantic_similarity": ç‚¹æ•°ï¼ˆ0-30ï¼‰,
  "emotional_tone": ç‚¹æ•°ï¼ˆ0-25ï¼‰,
  "character_consistency": ç‚¹æ•°ï¼ˆ0-25ï¼‰,
  "naturalness": ç‚¹æ•°ï¼ˆ0-20ï¼‰,
  "overall_score": å…¨ä½“ç‚¹æ•°ï¼ˆ0-100ï¼‰,
  "detailed_analysis": "å„é …ç›®ã®æ¸›ç‚¹ç†ç”±ã‚’å«ã‚€è©³ç´°ãªåˆ†æ"
}
```

ç”˜ã„æ¡ç‚¹ã§ã¯ãªãã€å³ã—ãæ­£ç¢ºãªè©•ä¾¡ã‚’ã—ã¦ãã ã•ã„ã€‚è©•ä¾¡ã¯å¿…ãšéŸ“å›½èªã§ã—ã¦ãã ã•ã„ã€‚"""

load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env.local'))

async def get_llm_response(user_message: str) -> str:
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤"""
    try:
        client = openai.AsyncOpenAI()
        
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message}
            ],
            max_tokens=200,
            temperature=0.7
        )
        
        content = response.choices[0].message.content
        return content.strip() if content else "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨åˆ†ã‹ã‚‰ãªã„ã§ã™ã€‚ã§ã‚‚ã€ã‚«ãƒŠã‚¿ã¯ã„ã¤ã§ã‚‚å‘³æ–¹ã ã‹ã‚‰ã­ã€‚"
    
    except Exception as e:
        print(f"LLM API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨åˆ†ã‹ã‚‰ãªã„ã§ã™ã€‚ã§ã‚‚ã€ã‚«ãƒŠã‚¿ã¯ã„ã¤ã§ã‚‚å‘³æ–¹ã ã‹ã‚‰ã­ã€‚"

async def evaluate_similarity_with_llm(user_input: str, expected_response: str, actual_response: str) -> dict:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìœ ì‚¬ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤"""
    try:
        client = openai.AsyncOpenAI()
        
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": SIMILARITY_EVALUATION_PROMPT},
                {"role": "user", "content": f"ì‚¬ìš©ì ì…ë ¥: {user_input}\nê¸°ëŒ€ ì‘ë‹µ: {expected_response}\nì‹¤ì œ ì‘ë‹µ: {actual_response}"}
            ],
            max_tokens=800,
            temperature=0.3
        )
        
        content = response.choices[0].message.content
        
        # JSON ë¶€ë¶„ ì¶”ì¶œ
        if content and "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            json_content = content[json_start:json_end].strip()
        else:
            json_content = content or ""
        
        try:
            result = json.loads(json_content)
            return result
        except json.JSONDecodeError:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {json_content}")
            return {
                "semantic_similarity": 0,
                "emotional_tone": 0,
                "character_consistency": 0,
                "naturalness": 0,
                "overall_score": 0,
                "detailed_analysis": "í‰ê°€ ì‹¤íŒ¨"
            }
    
    except Exception as e:
        print(f"LLM ìœ ì‚¬ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "semantic_similarity": 0,
            "emotional_tone": 0,
            "character_consistency": 0,
            "naturalness": 0,
            "overall_score": 0,
            "detailed_analysis": f"ì˜¤ë¥˜ ë°œìƒ: {e}"
        }

async def test_agent_responses_with_llm():
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ì‘ë‹µì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤"""
    # JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    with open("tests/kanata_response.json", "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    results = []
    total_scores = {
        "semantic_similarity": 0,
        "emotional_tone": 0,
        "character_consistency": 0,
        "naturalness": 0,
        "overall_score": 0
    }
    
    print("=== LLM ìœ ì‚¬ë„ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    print(f"ì´ {len(test_data)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
    
    for i, test_case in enumerate(test_data):
        user_message = test_case["user"]
        expected_response = test_case["expected_response"]
        
        print(f"\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}/{len(test_data)} ---")
        print(f"ì‚¬ìš©ì: {user_message}")
        print(f"ê¸°ëŒ€ ì‘ë‹µ: {expected_response}")
        
        # LLM ì‘ë‹µ ìƒì„±
        print("ì‹¤ì œ ì‘ë‹µ ìƒì„± ì¤‘...")
        actual_response = await get_llm_response(user_message)
        print(f"ì‹¤ì œ ì‘ë‹µ: {actual_response}")
        
        # LLMìœ¼ë¡œ ìœ ì‚¬ë„ í‰ê°€
        print("LLM ìœ ì‚¬ë„ í‰ê°€ ì¤‘...")
        similarity_result = await evaluate_similarity_with_llm(
            user_message, expected_response, actual_response
        )
        
        result = {
            "case_number": i + 1,
            "user_input": user_message,
            "expected_response": expected_response,
            "actual_response": actual_response,
            "evaluation": similarity_result,
            "notes": test_case.get("notes", {})
        }
        
        results.append(result)
        
        # ì ìˆ˜ ëˆ„ì 
        for key in total_scores:
            total_scores[key] += similarity_result.get(key, 0)
        
        print(f"ì „ì²´ ì ìˆ˜: {similarity_result.get('overall_score', 0):.1f}/100")
        
        # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (rate limit ë°©ì§€)
        await asyncio.sleep(2)
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    num_cases = len(test_data)
    average_scores = {key: score / num_cases for key, score in total_scores.items()}
    
    return results, average_scores

def save_llm_test_results(results: list, average_scores: dict):
    """LLM í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    test_results = {
        "test_summary": {
            "total_cases": len(results),
            "average_scores": average_scores,
            "evaluation_method": "LLM-based similarity evaluation"
        },
        "test_cases": results
    }
    
    # ê²°ê³¼ ì €ì¥
    with open("tests/llm_similarity_results.json", "w", encoding="utf-8") as f:
        json.dump(test_results, f, ensure_ascii=False, indent=2)
    
    print(f"\ní…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ tests/llm_similarity_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=== LLM ê¸°ë°˜ ìºë¦­í„° ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    # LLM ìœ ì‚¬ë„ í‰ê°€ í…ŒìŠ¤íŠ¸
    results, average_scores = await test_agent_responses_with_llm()
    
    print("\n" + "="*50)
    print("=== ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print("="*50)
    
    print(f"í‰ê·  ì˜ë¯¸ì  ìœ ì‚¬ë„: {average_scores['semantic_similarity']:.1f}/30")
    print(f"í‰ê·  ê°ì •ì  í†¤: {average_scores['emotional_tone']:.1f}/25")
    print(f"í‰ê·  ìºë¦­í„° ì¼ê´€ì„±: {average_scores['character_consistency']:.1f}/25")
    print(f"í‰ê·  ìì—°ìŠ¤ëŸ¬ì›€: {average_scores['naturalness']:.1f}/20")
    print(f"ì „ì²´ í‰ê·  ì ìˆ˜: {average_scores['overall_score']:.1f}/100")
    
    print("\n=== ê²°ê³¼ í•´ì„ ===")
    overall_avg = average_scores['overall_score']
    
    if overall_avg >= 80:
        print("âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥: LLM í‰ê°€ ì ìˆ˜ê°€ 80ì  ì´ìƒì…ë‹ˆë‹¤.")
    elif overall_avg >= 60:
        print("âš ï¸ ë³´í†µ ì„±ëŠ¥: LLM í‰ê°€ ì ìˆ˜ê°€ 60-80ì  ì‚¬ì´ì…ë‹ˆë‹¤.")
    else:
        print("âŒ ê°œì„  í•„ìš”: LLM í‰ê°€ ì ìˆ˜ê°€ 60ì  ë¯¸ë§Œì…ë‹ˆë‹¤.")
    
    print(f"\nğŸ’¡ LLM ê¸°ë°˜ í‰ê°€ëŠ” ì˜ë¯¸, ê°ì •, ìºë¦­í„° ì¼ê´€ì„±, ìì—°ìŠ¤ëŸ¬ì›€ì„ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥
    save_llm_test_results(results, average_scores)
    
    return results, average_scores

if __name__ == "__main__":
    asyncio.run(main()) 